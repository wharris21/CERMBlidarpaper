---
title: "Sense Checking"
output: html_notebook
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(here)
library(tidyverse, warn.conflicts = FALSE)
library(ggplot2)

library(sf)
library(terra)
#library(gdalraster) #for building raster table, not required

```

```{r paths}
#Data paths
## Path to Dry Sclerophyll forest classes
DSF_CLASS_PATH <- here::here("data/dsf_classes.tif")
##Path to masked Dry Sclerophyll forest classes with attribute table
DSF_MASK_PATH <- here::here("data/dsf_classes_with_lidar.tif")

##Path to Lidar Metadata
LIDAR_METADATA_PATH <- here::here("data/lidar_metadata.gpkg")
##Path to rasterised Lidar metadata
RLIDAR_PATH <- here::here("data/lidar_coverage_3308.tif")

##Path to NPWS estates
NPWS_ESTATES_PATH <- here::here("data/npws_estates/NPWS_Estate.shp")
##Path to NPWS fire history
NPWS_FIREHIST_PATH <- here::here("data/NPWS_firehistory/NPWSFireHistory.shp")

##Path to sample points
SAMPLE_POINTS_PATH <- here("data/sample_points_200.shp")
```

Calculating percent coverage of DSF classes within Lidar extent

```{r}
#Get attribute tables as dataframes and remove RGB values
dsf_classes_df <- cats(dsf_classes) %>% 
  as.data.frame()

dsf_classes_df <- subset(dsf_classes_df, select = -c(R,G,B))

dsf_lidar_df <- cats(dsf_classes_lidar) %>% 
  as.data.frame()

dsf_lidar_df <- subset(dsf_lidar_df, select = -c(R,G,B,A))

#Set colnames to match for left_join
colnames(dsf_lidar_df) <- c("VALUE", "VEGCLASS", "LIDARCOUNT")

#leftjoin and calculate count difference and percentage remaining of each class
dsf_classes_comp <- left_join(dsf_classes_df, dsf_lidar_df) %>% 
  select(VALUE, VEGCLASS, COUNT, LIDARCOUNT) %>% 
  mutate(
    count_dif = COUNT - LIDARCOUNT,
    percent_remain = (LIDARCOUNT/COUNT)*100,
    area_reamain = ((LIDARCOUNT*(5^2))/1000000)
  )

head(dsf_classes_comp)

```

Data pre-processing

```{r}

#Masking dry sclerophyll raster to rasterised lidar extent

#Crop rlidar raster to match extent of dsf_classes raster. Had to do multiple crops to get it working
##Crop dsf_classes raster to lidar_metadata extent
dsf_classes_lidar <- terra::crop(dsf_classes, lidar_metadata)

##crop rasterised lidar_metadata to cropped dsf classes raster 
rlidar_crop <- teraa::crop(rlidar, dsf_classes_lidar)

#mask dsf_crop raster with cropped, rasterised lidar and write to disk
rdsf_mask <- terra::mask(dsf_classes_lidar, rlidar_crop)
terra::writeRaster(rdsf_mask, here::here("data/rdsf_mask.tif"))

#Mask NPWS estates to Lidar extent

#Read npws_estate shapefile
npws_estates <- terra::vect(NPWS_ESTATES_PATH)
#Read lidar metadata shapefiles
lidar_metadata <- terra::vect(LIDAR_METADATA_PATH)

#Mask NPWS estates to lidar holdings
##Ensure both are in the same CRS
npws_estates <- terra::project(npws_estates, crs(lidar_metadata))

##Mask
npws_estates_mask <- terra::mask(npws_estates, lidar_metadata)
```

Creating sample points with 200m distance over the extent of NPWS estates within the lidar holdings coverage

```{r}
gc()

create_sample_points <- function(SpatVector, distance){
  
  print(paste0("inter point distance set to ", distance,(" meters!")))
  
  sample_distance <- c(distance, distance)
  
    #Read lidar_metadata shp
    sp_vect <- SpatVector
    
    #Create raster from lidar_metadata.shp with a resolution with desired inter-point distance
    sp_template <- rast(sp_vect, resolution = sample_distance)
    
    #set some default values (may not be needed)
    values(sp_template) <- 1:ncell(sp_template)
    
    #mask sp_template raster to the lidar .shp to ensure no out-of-area points
    sp_template <- mask(sp_template, sp_vect)
    
    #Set raster to points (defaults to centroids)
    sp <- as.points(sp_template, na.rm = TRUE)
    
    #Intersect to only keep points in shape
    sp <- sp[sp_vect]
    
    print(paste0("Created ", length(sp), " points! Wow! That's a lot!"))
  
  return(sp)
}

#create sample points and write to disk
sample_points <- create_sample_points(npws_estates_mask, 2000)
terra::writeVector(sample_points, here::here("data/sample_points_2000.shp"), overwrite=TRUE)
```

Extract data

```{r}
vect_vect_extract <- function(points, polygons){
  
  polygons_trans <- st_transform(polygons, st_crs(points))
  out <- st_join(points, polygons_trans, join = st_intersects)
  return(out)
}

#Read in sample points, remove lyr.1 layer and add pointID
sample_points <- st_read(SAMPLE_POINTS_PATH)
sample_points$lyr.1 <- NULL
sample_points$pointID <- 1:nrow(sample_points)

#Read fire history and validate polygons
npws_firehistory <- st_read(NPWS_FIREHIST_PATH)

#Remove inavalid dates
npws_firehistory <- npws_firehistory %>% 
  mutate(StartDate := if_else(StartDate < as.Date("1900-01-01"), NA, StartDate)) %>% 
  mutate(EndDate := if_else(EndDate < as.Date("1900-01-01"), NA, EndDate))

#create fire id column
npws_firehistory$fireID <- 1:nrow(npws_firehistory)

#Extract fire history
npws_extract <- vect_vect_extract(sample_points, npws_firehistory)

#Extract lidar history
lidar_metadata <- st_read(LIDAR_METADATA_PATH)
lidar_extract <- vect_vect_extract(sample_points, lidar_metadata)

#extract dsf classes
dsf_classes <- terra::rast(DSF_MASK_PATH)
dsf_extract <- terra::extract(dsf_classes, sample_points, ID = FALSE, bind = TRUE) %>% 
  st_as_sf()

#create empty data frame
data <- st_join(npws_extract, lidar_extract)

data <- st_join(data, dsf_extract)

#Drop rows outside DSF range
data <- data[!is.na(data$Class),]

#remove extra pointID columns
data$pointID.x <- NULL
data$pointID.y <- NULL

#fire year is included with fire type as a variable, these need to be split by substring fire year
##Get fire data for use in calculating TSF
data$firedate <- paste0((substr(data$Label, 1, 4)),"-12-31") %>% 
                          as.Date()

##Simpler fire year for use in later rainfall calculations or anything else that doesn't need a date format
data$fireyear <- substr(data$Label, 1, 4)

data$firetype <- substr(data$Label, 9, 30) %>% 
  as.factor()

#Get lidar capture start as date 
data$ldr_date <- as.Date(data$capture_end)

#calculate tsf
data$tsf_months <- -(interval(ymd(data$ldr_date), ymd(data$firedate)) %/% months(1))

#filter for the most recent fire at each point
data <- data %>% 
  group_by(pointID) %>% 
  filter(firedate == max(firedate)) %>% 
  ungroup()

#filter for only wildfires and where firedate is before the most recent lidar
data <- data %>% 
  filter(firetype == "Wildfire") %>% 
  filter(firedate < ldr_date) %>% 
  filter(tsf_months > 4)

#remove duplicated points
data <- data %>% 
  distinct(pointID, .keep_all = TRUE)

st_write(data, here("data/data.gpkg"), driver = "GPKG", append = FALSE)

```

Functions for calculating post-fire rainfall data

```{r}
extract_raster_data_by_year <- function(sf, year_column, raster_list) {
  
  #Get crs of rasters
  target_crs <- terra::rast(raster_list[1]) %>% 
    crs()
  
  original_crs <- st_crs(sf) 
  
  sf <- st_transform(sf, target_crs)
  
  # Get unique years from the data
  unique_years <- unique(sf[[year_column]])
  
  #create naming convention
  prefire_names <- paste0("prefrain.", month.abb)
  postfire_names <- paste0("postfrain.", month.abb)
  postfire1_names <- paste0("postfrain1.", month.abb)
  
  firenamestring <- c(prefire_names,postfire_names,postfire1_names)
  
  # Initialize a df to store extracted data
  results <- data.frame()
  
  # Loop over each year
  for (year in unique_years) {
    # Subset data for the current year
    subset_data <- sf %>% filter(!!sym(year_column) == year)
    
    #get points for data subset
    points <- st_geometry(subset_data) %>% 
      st_as_sf()
    
    #Get years
    prefire <- as.numeric(year) - 1
    postfire <- as.numeric(year)
    postfire1 <- as.numeric(year) + 1
    
    # Find the raster file path corresponding to each raster year
    prefire_rast <- raster_list[grep(prefire, raster_list)]
    postfire_rast <- raster_list[grep(postfire, raster_list)]
    postfire1_rast <- raster_list[grep(postfire1, raster_list)]
    
    # Read the raster file
    raster_data <- terra::rast(c(prefire_rast, postfire_rast, postfire1_rast))
      
    # Extract data from the raster using the provided points
    e <- terra::extract(raster_data, points)
    e[1] <- NULL
    colnames(e) <- firenamestring
    e$year <- year
    results <- bind_rows(results, e)
  }
  
  results$geometry <- st_geometry(sf)
  results <- bind_cols(results, sf$pointID)
  
  
  #rename final column to pointID
  results <- results %>% 
    rename_at(ncol(results), ~"pointID")
  
  results <- st_transform(st_as_sf(results), original_crs)
  
  return(results)
}

#Get list of rainfall raster files
raster_list <- list.files(here("data/AGCD_monthly_rainfall"), full.names = TRUE)

#read data table
data <- st_read(here("data/data.gpkg"))

# Extract raster data for all years
rainfall_data <- extract_raster_data_by_year(data, "fireyear", raster_list)

st_write(rainfall_data, here("data/rainfall_only.gpkg"), append = FALSE)

#Create longform rainfalldata
rainfall_data$year <- as.numeric(rainfall_data$year)
rainfall_long <- rainfall_data %>% 
  pivot_longer(
    cols = !c("pointID","year","geometry"), 
    names_to = c("month"),
    values_to = "rainfall"
)

data_rainfall <- left_join(data, as.data.frame(rainfall_data), by = "pointID")

st_write(data_rainfall, here("data/data_rainfall.gpkg"), driver = "GPKG", append = FALSE)
```


```{r}
#create summary table
#number of points per class
data_sum <- data %>% 
  group_by(class) %>% 
  count(class)

ymax <- data %>% 
  group_by(class) %>% 
  summarize(max(y)) %>% 
  ungroup()

ymin <- data %>% 
  group_by(class) %>% 
  summarize(min(y)) %>% 
  ungroup()

xmax <- data %>% 
  group_by(class) %>% 
  summarize(max(x)) %>% 
  ungroup()
  #summarize(min(y))

xmin <- data %>% 
  group_by(Class) %>% 
  summarize(min(x)) %>% 
  ungroup()

data_sum <- left_join(data_sum, ymax, by = "class")
data_sum <- left_join(data_sum, ymin, by = "class")
data_sum <- left_join(data_sum, xmax, by = "class")
data_sum <- left_join(data_sum, xmin, by = "class")

colnames(data_sum) <- c("class", "n", "ymax", "ymin", "xmax", "xmin")

data_sum <- data_sum %>% 
  mutate(
    ydif = (ymax - ymin)/1000,
    xdif = (xmax - xmin)/1000)

write.csv(data_sum, here("data/datasum_table.csv"))

#add columns for number of sample points with no fire
#add na value (maybe -1) for long unburnt values
#Check tsf distribution for each forest class
#Thinking about what to do with points with less than 2 years between fire and lidar (makes post-fire rainfall tricky)


```

