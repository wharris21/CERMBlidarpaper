---
title: "Sense Checking"
output: html_notebook
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(here)
library(tidyverse, warn.conflicts = FALSE)
library(ggplot2)

library(terra)
#library(gdalraster) #for building raster table, not required

```

```{r paths}
#Data paths
## Path to Dry Sclerophyll forest classes
DSF_CLASS_PATH <- here::here("data/dsf_classes.tif")
##Path to masked Dry Sclerophyll forest classes with attribute table
DSF_MASK_PATH <- here::here("data/dsf_classes_with_lidar.tif")

##Path to Lidar Metadata
LIDAR_METADATA_PATH <- here::here("data/lidar_metadata.gpkg")
##Path to rasterised Lidar metadata
RLIDAR_PATH <- here::here("data/lidar_coverage_3308.tif")

##Path to NPWS estates
NPWS_ESTATES_PATH <- here::here("data/npws_estates/NPWS_Estate.shp")
##Path to NPWS fire history
NPWS_FIREHIST_PATH <- here::here("data/NPWS_firehistory/NPWSFireHistory.shp")
```

```{r}
#Read data files
#Read dsf classes
dsf_classes <- terra::rast(DSF_CLASS_PATH)
dsf_classes_lidar <- terra::rast(DSF_MASK_PATH)

#read lidar metadata
lidar_metadata <- terra::vect(LIDAR_METADATA_PATH)

#Read rasterised lidar
rlidar <- terra::rast(RLIDAR_PATH)

```

Code chunk for doing random checks

```{r}
#Quick check to sample the which class is most common
set.seed(1)
pts <- terra::spatSample(as.polygons(lidar_metadata), 10000)
test <- terra::extract(dsf_classes, pts)

ggplot(na.omit(test), aes(VEGCLASS)) + geom_bar() + scale_x_discrete(guide = guide_axis(angle = 90))

```

Masking dry sclerophyll raster to rasterised lidar extent

```{r}
#Crop rlidar raster to match extent of dsf_classes raster. Had to do multiple crops to get it working
##Crop dsf_classes raster to lidar_metadata extent
dsf_classes_lidar <- terra::crop(dsf_classes, lidar_metadata)

##crop rasterised lidar_metadata to cropped dsf classes raster 
rlidar_crop <- teraa::crop(rlidar, dsf_classes_lidar)

#mask dsf_crop raster with cropped, rasterised lidar and write to disk
rdsf_mask <- terra::mask(dsf_classes_lidar, rlidar_crop)
terra::writeRaster(rdsf_mask, here::here("data/rdsf_mask.tif"))
```

Creating new attribute table with gdalraster (see DSF_class_raster.rmd). Turns out QGIS is a lot faster than this.

```{r}
#Read masked rdsf file
#Read masked rdsf file
rdsf_mask <- terra::rast(DSF_MASK_PATH)
dsf_classes <- terra::rast(DSF_CLASS_PATH)

#get cats list
rdsf_mask_classes <- terra::cats(rdsf_mask)

#make new attribute table using gdalraster
gdsf_mask <- new(GDALRaster, DSF_MASK_PATH, read_only = FALSE)

dsf_colours <- palette.colors(length(rdsf_mask), palette = "Alphabet") %>%
  col2rgb() %>%
  t()

dat_rat <- data.frame(VALUE = 1:length(rdsf_mask_classes),
                      VEGCLASS = rdsf_mask_classes,
                      R = dsf_colours[, "red"],
                      G = dsf_colours[, "green"],
                      B = dsf_colours[, "blue"])

# Note: This will take a few minutes
tbl <- gdalraster::buildRAT(gdsf_mask, table_type = "thematic", na_value = -9999, join_df = dat_rat)

gdsf_mask$setDefaultRAT(band = 1, tbl)
gdsf_mask$flushCache()
```

Calculating percent coverage of DSF classes within Lidar extent

```{r}
#Get attribute tables as dataframes and remove RGB values
dsf_classes_df <- cats(dsf_classes) %>% 
  as.data.frame()

dsf_classes_df <- subset(dsf_classes_df, select = -c(R,G,B))

dsf_lidar_df <- cats(dsf_classes_lidar) %>% 
  as.data.frame()

dsf_lidar_df <- subset(dsf_lidar_df, select = -c(R,G,B,A))

#Set colnames to match for left_join
colnames(dsf_lidar_df) <- c("VALUE", "VEGCLASS", "LIDARCOUNT")

#leftjoin and calculate count difference and percentage remaining of each class
dsf_classes_comp <- left_join(dsf_classes_df, dsf_lidar_df) %>% 
  select(VALUE, VEGCLASS, COUNT, LIDARCOUNT) %>% 
  mutate(
    count_dif = COUNT - LIDARCOUNT,
    percent_remain = (LIDARCOUNT/COUNT)*100,
    area_reamain = ((LIDARCOUNT*(5^2))/1000000)
  )

head(dsf_classes_comp)

```

Prepare NPWS estate layer by masking to lidar metadata extent

```{r}
gc()

#Read npws_estate shapefile
npws_estates <- terra::vect(NPWS_ESTATES_PATH)
#Read lidar metadata shapefiles
lidar_metadata <- terra::vect(LIDAR_METADATA_PATH)

#Mask NPWS estates to lidar holdings
##Ensure both are in the same CRS
npws_estates <- terra::project(npws_estates, crs(lidar_metadata))

##Mask
npws_estates_mask <- terra::mask(npws_estates, lidar_metadata)

```

Creating sample points with 200m distance over the extent of NPWS estates within the lidar holdings coverage

```{r}
gc()

create_sample_points <- function(SpatVector, distance){
  
  print(paste0("inter point distance set to ", distance,(" meters!")))
  
  sample_distance <- c(distance, distance)
  
    #Read lidar_metadata shp
    sp_vect <- SpatVector
    
    #Create raster from lidar_metadata.shp with a resolution with desired inter-point distance
    sp_template <- rast(sp_vect, resolution = sample_distance)
    
    #set some default values (may not be needed)
    values(sp_template) <- 1:ncell(sp_template)
    
    #mask sp_template raster to the lidar .shp to ensure no out-of-area points
    sp_template <- mask(sp_template, sp_vect)
    
    #Set raster to points (defaults to centroids)
    sp <- as.points(sp_template, na.rm = TRUE)
    
    #Intersect to only keep points in shape
    sp <- sp[sp_vect]
    
    print(paste0("Created ", length(sp), " points! Wow! That's a lot!"))
  
  return(sp)
}
```

Create sample points over NPWS estates

```{r}
sample_points <- create_sample_points(npws_estates_mask, 200)
terra::writeVector(sample_points, here::here("data/sample_points_200.shp"), overwrite=TRUE)
sample_point_path <- here("data/sample_points_200.shp")
```

Extract using sample points

```{r}
#function to check CRS and extract point data from rasters or vectors
point_extract <- function(points, data){
  s <- points
  r <- data
  if (crs(s) != crs(r)) {
  message("CRS doesn't match")
  s <- terra::project(s, crs(r))
  message("Points reprojected!")
  }
  message("Running extract!")
  out <- terra::extract(r, s)
  message(paste0("Successfully extracted ",nrow(out)," points!"))
  out
}

#Read NPWS fire history
npws_firehistory <- terra::vect(NPWS_FIREHIST_PATH)

#read sample point shp
sample_point_path <- here("data/sample_points.shp")
sample_points <- terra::vect(sample_point_path)

#extract npws fire history
npws_firehistory <- terra::vect(NPWS_FIREHIST_PATH)
npws_fire_extract <- point_extract(sample_points, npws_firehistory)
colnames(npws_fire_extract)[1]

#Extract lidar data and rename id.y column to ID
lidar_metadata <- terra::vect(LIDAR_METADATA_PATH)
lidar_extract <- point_extract(sample_points, lidar_metadata)
colnames(lidar_extract)[1] <- "ID"

#Extract vegclass
##Read dsf class raster
dsf_class_mask <- terra::rast(DSF_MASK_PATH)
dsf_class_extract <- point_extract(sample_points, dsf_class_mask)

#Create data table df
data <- npws_fire_extract
colnames(data)[1] <- "ID"

#join data and lidar extracts
data <- dplyr::left_join(data, lidar_extract, by = "ID")

#join data and firehistory extracts
data <- left_join(data, dsf_class_extract, by = "ID")

#create and join point coordinates
point_crds <- crds(sample_points, df = TRUE)
point_crds <- tibble::rowid_to_column(point_crds, "ID")
data <- left_join(data, point_crds, by = "ID")

#Remove rows that are outside of DSF range
data <- data[!is.na(data$Class),]

#fire year is included with fire type as a variable, these need to be split by substring fire year
data$fireyear <- paste0((substr(data$Label, 1, 4)),"-12-31") %>% 
                          as.Date()
data$firetype <- substr(data$Label, 9, 30) %>% 
  as.factor()

#Get lidar capture start as date 
data$lidar_capture_date <- as.Date(data$capture_end)

#Filter the last wildfire prior to the most recent lidar pass
data <- data %>% 
  group_by(ID) %>% 
   
  dplyr::filter(fireyear < lidar_capture_date) %>% #Keep fire that occurred before most recent lidar pass
  dplyr::filter(fireyear == max(fireyear)) %>% #Keep most recent fire
  dplyr::filter(firetype == "Wildfire") %>% #Keep only wildfires
   
  ungroup()

#Remove duplicate fires
data <- data %>% 
  distinct(ID, .keep_all = TRUE)

#calculate tsf
data$tsf_months <- -(interval(ymd(data$lidar_capture_date), ymd(data$fireyear)) %/% months(1))
```

```{r}
RAINFALL_PATH <- here::here("data/AGCD_monthly_rainfall")

#Query rainfall data
get_rainfall_data <- function(){
  
  #create rainfall_data df and get first and second years
  rainfall_data <- data %>%
  select("ID", "fireyear") %>% 
  mutate(year1 = substr(
    fireyear + years(1),
    1, 4),
    
    year2 = substr(
    fireyear + years(2),
    1, 4)
    )
  
  # Initialize empty data frames to store file paths for each year
  year1df <- data.frame()
  year2df <- data.frame()
  out <- data.frame()
  
  # Loop through each pattern in the 'year1' column of 'rainfall_data'
  for (i in rainfall_data$year1){
     
    # Get file paths that match the pattern for year 1
    year1filepath <- list.files(RAINFALL_PATH, pattern = i, full.names = TRUE)
    
    # Append the file paths to the 'year1df' data frame
    year1df <- rbind(year1df, year1filepath)
  }
  
  # Loop through each pattern in the 'year2' column of 'rainfall_data'
  for (i in rainfall_data$year2){
     
    # Get file paths that match the pattern for year 2
    year2filepath <- list.files(RAINFALL_PATH, pattern = i, full.names = TRUE)
    
    # Append the file paths to the 'year2df' data frame
    year2df <- rbind(year2df, year2filepath)
  }
  
  # Combine 'year1df' and 'year2df'
  out <- cbind(year1df, year2df)
  
  # Assign column names to the output data frame
  colnames(out) <- c("year1path", "year2path")
  
  # Bind outputs and return the output data frame
  rainfall_data <- bind_cols(rainfall_data, out)
  
  rainfall_data
}

rainfall_data <- get_rainfall_data()


test <- rainfall_data %>% 
  select("ID", "year1path") %>% 
  group_by(year1path) %>% 
  






#Create a list of years


#Extract the rainfall data for all months in given years
#Remove unnecessary months
#Add rainfall together
#Create output columns

```

g```{r}
#create summary table
#number of points per class
data_sum <- data %>% 
  group_by(Class) %>% 
  count(Class)

ymax <- data %>% 
  group_by(Class) %>% 
  summarize(max(y)) %>% 
  ungroup()
  #summarize(min(y))

ymin <- data %>% 
  group_by(Class) %>% 
  summarize(min(y)) %>% 
  ungroup()

data_sum <- left_join(data_sum, ymax, by = "Class")
data_sum <- left_join(data_sum, ymin, by = "Class")

colnames(data_sum) <- c("class", "n", "ymax", "ymin")

data_sum <- data_sum %>% 
  mutate(
    ydif = (ymax - ymin)/1000)
```

